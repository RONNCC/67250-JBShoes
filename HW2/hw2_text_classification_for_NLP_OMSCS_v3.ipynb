{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PwKFZCfqz1c",
        "outputId": "92a89983-dfbb-4f53-814e-8c8f184e2773"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets==2.16.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.16.1)\n",
            "Requirement already satisfied: gensim==4.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.3.2)\n",
            "Requirement already satisfied: gradescope-utils==0.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.5.0)\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: matplotlib==3.7.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.7.4)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.8.1)\n",
            "Requirement already satisfied: numpy==1.26.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.26.2)\n",
            "Requirement already satisfied: pandas==2.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.1.4)\n",
            "Requirement already satisfied: papermill==2.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.5.0)\n",
            "Requirement already satisfied: scikit-learn==1.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.3.2)\n",
            "Requirement already satisfied: subprocess32==3.5.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (3.5.4)\n",
            "Requirement already satisfied: torch==2.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.1.2)\n",
            "Requirement already satisfied: torchviz==0.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (0.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1->-r requirements.txt (line 1)) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1->-r requirements.txt (line 1)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1->-r requirements.txt (line 1)) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1->-r requirements.txt (line 1)) (0.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1->-r requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1->-r requirements.txt (line 1)) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1->-r requirements.txt (line 1)) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1->-r requirements.txt (line 1)) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1->-r requirements.txt (line 1)) (0.23.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1->-r requirements.txt (line 1)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2->-r requirements.txt (line 2)) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2->-r requirements.txt (line 2)) (7.0.4)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 4)) (6.5.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 4)) (5.5.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 4)) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 4)) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 4)) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 4)) (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.4->-r requirements.txt (line 5)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.4->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.4->-r requirements.txt (line 5)) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.4->-r requirements.txt (line 5)) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.4->-r requirements.txt (line 5)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.4->-r requirements.txt (line 5)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.4->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r requirements.txt (line 6)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r requirements.txt (line 6)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r requirements.txt (line 6)) (2024.5.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.1.4->-r requirements.txt (line 8)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.1.4->-r requirements.txt (line 8)) (2024.1)\n",
            "Requirement already satisfied: nbformat>=5.1.2 in /usr/local/lib/python3.10/dist-packages (from papermill==2.5.0->-r requirements.txt (line 9)) (5.10.4)\n",
            "Requirement already satisfied: nbclient>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from papermill==2.5.0->-r requirements.txt (line 9)) (0.10.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from papermill==2.5.0->-r requirements.txt (line 9)) (0.4)\n",
            "Requirement already satisfied: tenacity>=5.0.2 in /usr/local/lib/python3.10/dist-packages (from papermill==2.5.0->-r requirements.txt (line 9)) (8.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.2->-r requirements.txt (line 10)) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 12)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 12)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 12)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 12)) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 12)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 12)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 12)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 12)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 12)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 12)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 12)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 12)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 12)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 12)) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 12)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 12)) (2.1.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz==0.0.2->-r requirements.txt (line 13)) (0.20.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->-r requirements.txt (line 12)) (12.5.40)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 1)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 1)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 1)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.2.0->papermill==2.5.0->-r requirements.txt (line 9)) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.2.0->papermill==2.5.0->-r requirements.txt (line 9)) (5.7.2)\n",
            "Requirement already satisfied: traitlets>=5.4 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.2.0->papermill==2.5.0->-r requirements.txt (line 9)) (5.7.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1.2->papermill==2.5.0->-r requirements.txt (line 9)) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1.2->papermill==2.5.0->-r requirements.txt (line 9)) (4.19.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.4->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.16.1->-r requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.16.1->-r requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.16.1->-r requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.16.1->-r requirements.txt (line 1)) (2024.6.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim==4.3.2->-r requirements.txt (line 2)) (1.14.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 4)) (7.34.0)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 4)) (6.3.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 4)) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 4)) (3.0.11)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.2->-r requirements.txt (line 12)) (2.1.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 4)) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 4)) (2.16.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 4)) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 4)) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 4)) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 4)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 4)) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 4)) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 4)) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 4)) (2.4.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2->-r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 4)) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 4)) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 4)) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 4)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 4)) (4.9.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill==2.5.0->-r requirements.txt (line 9)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill==2.5.0->-r requirements.txt (line 9)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill==2.5.0->-r requirements.txt (line 9)) (0.18.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbclient>=0.2.0->papermill==2.5.0->-r requirements.txt (line 9)) (4.2.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (0.2.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 4)) (0.2.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->-r requirements.txt (line 4)) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 4)) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 4)) (0.8.4)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 4)) (1.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "ywvLShfXIKS1",
        "tags": []
      },
      "source": [
        "# NLP Classification\n",
        "\n",
        "In this assignment we look at several ways of classifying texts:\n",
        "- Naive Bayes\n",
        "- Logistic Regression\n",
        "- Multinomial Regression\n",
        "\n",
        "We also look at binary label classification problems (e.g., sentiment analysis) and multinomial classification problems (e.g., topic analysis).\n",
        "\n",
        "We will use two datasets:\n",
        "- [IMDb movie review sentiment](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
        "- [AG News topics](https://huggingface.co/datasets/ag_news)\n",
        "\n",
        "**Tips:**\n",
        "- Read all the code. We don't ask you to write the training loops, or evaluation loops, but it is often instructive to see how the models are trained and evaluated.\n",
        "- If you have a model that is learning (loss is decreasing), but you want to increase accuracy, try using ``nn.Dropout`` layers just before the final linear layer to force the model to handle missing or unfamiliar data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "editable": false,
        "tags": [
          "raises-exception"
        ],
        "id": "UAUi78G_qseE"
      },
      "outputs": [],
      "source": [
        "# start time - notebook execution\n",
        "import time\n",
        "start_nb = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "1xVv6McXodZG",
        "tags": []
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "vMRiPTv7vHe2",
        "tags": []
      },
      "source": [
        "Import packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "editable": false,
        "id": "S5VeRBOiIKS4",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "editable": false,
        "tags": [],
        "id": "43OV-YJdqseF"
      },
      "source": [
        "# Initialize the Autograder"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjHuNRTZqySx"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "editable": false,
        "tags": [
          "raises-exception"
        ],
        "id": "l89tW43dqseF"
      },
      "outputs": [],
      "source": [
        "# import the autograder tests\n",
        "import hw2_tests as ag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "qyRo0v0JIKTK",
        "tags": []
      },
      "source": [
        "# Functions for cleaning up raw texts and tokenizing the corpus\n",
        "\n",
        "We perform text preprocessing that includes: removing HTML tags, making text lower case, stemming, and disposing of stopwords.\n",
        "In the end, we will split the entire dataset into training, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "editable": false,
        "id": "JvfvVWHyIKTL",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "# Stemming the text\n",
        "def simple_stemmer(text):\n",
        "    ps=nltk.porter.PorterStemmer()\n",
        "    text= [ps.stem(word) for word in text]\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "editable": false,
        "id": "Wr_LdWWoIKTL",
        "tags": [
          "raises-exception"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "793d3b5a-e6e9-4ecd-c9f4-0c84298730f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n"
          ]
        }
      ],
      "source": [
        "stopwords_english = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
        "print(stopwords_english)\n",
        "\n",
        "#removing the stopwords\n",
        "def remove_stopwords(text, stopword_list):\n",
        "    tokens = [token.strip() for token in text]\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    return filtered_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "editable": false,
        "id": "NCXHv_tZIKTM",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "def tokenize_and_clean(line, stem_and_remove_stop_words = True):\n",
        "\n",
        "    line = re.sub(r\"<.*?>\", \"\", line).strip() # remove all HTML tags\n",
        "    line = re.sub(r'[^a-zA-Z0-9]', ' ', line) # remove punc\n",
        "    line = line.lower().split()  # lower case\n",
        "    if stem_and_remove_stop_words:\n",
        "        line = remove_stopwords(line, stopwords_english)\n",
        "        line = simple_stemmer(line)\n",
        "\n",
        "    return line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "ohc6u_lDIKS-",
        "tags": []
      },
      "source": [
        "# Download and unpack the sentiment data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "zIklHox9vjMI",
        "tags": []
      },
      "source": [
        "We are using IMDb Dataset for binary sentiment classification that provides a set of 25K highly polar reviews for training, and 25K for testing\n",
        "(each set contains an equal number of positive and negative examples).\n",
        "\n",
        "Dataset folder structure is as follows:\n",
        "\n",
        "dataset/ \\\n",
        "├── test/ \\\n",
        "│     ├── pos/ \\\n",
        "│     ├── neg/ \\\n",
        "├── train/ \\\n",
        "      ├── pos/ \\\n",
        "      └── neg/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "editable": false,
        "tags": [
          "raises-exception"
        ],
        "id": "ICR6QyZbqseG"
      },
      "outputs": [],
      "source": [
        "# check if dataset is downloaded\n",
        "if not os.path.isfile('aclImdb_v1.tar'):\n",
        "    print(\"Downloading dataset...\")\n",
        "    !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "    !gunzip aclImdb_v1.tar.gz\n",
        "    !tar -xvf aclImdb_v1.tar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "GRD03KapvnI0",
        "tags": []
      },
      "source": [
        "Load in the text from the folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "editable": false,
        "id": "HI3VdrTnIKTO",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "def load_text_from_folders(path, file_list, dataset, samples = 2000, stem_and_remove_stop_words = True):\n",
        "    \"\"\"Read set of files from given directory and save returned lines to list.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : str\n",
        "        Absolute or relative path to given file (or set of files).\n",
        "    file_list: list\n",
        "        List of files names to read.\n",
        "    dataset: list\n",
        "        List that stores read lines.\n",
        "    samples: int\n",
        "        Number of samples in the output\n",
        "    \"\"\"\n",
        "    for i, file in enumerate(file_list):\n",
        "        if i >= samples:\n",
        "            break\n",
        "        with open(os.path.join(path, file), 'r', encoding='utf8') as text:\n",
        "            contents = text.read()\n",
        "            contents_tokenized = tokenize_and_clean(contents, stem_and_remove_stop_words=stem_and_remove_stop_words)\n",
        "            dataset.append(contents_tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "_8KuCNoXIKTO",
        "tags": []
      },
      "source": [
        "# Creating training and test sets\n",
        "\n",
        "This creates four arrays:\n",
        "\n",
        "\n",
        "*   ```train_pos``` -- instances in the training set with positive sentiment labels\n",
        "*   ```train_neg``` -- instances in the training set with negative sentiment labels\n",
        "*   ```test_pos``` -- instances in the testing set with positive sentiment labels\n",
        "*   ```test_neg``` -- instances in the testing set with negative sentiment labels\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "editable": false,
        "id": "4Nv5FvBIIKTO",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "# Path to dataset location\n",
        "path = 'aclImdb/'\n",
        "\n",
        "# Create lists that will contain read lines\n",
        "train_pos, train_neg, test_pos, test_neg = [], [], [], []\n",
        "\n",
        "# Create a dictionary of paths and lists that store lines (key: value = path: list)\n",
        "sets_dict = {'train/pos/': train_pos, 'train/neg/': train_neg,\n",
        "             'test/pos/': test_pos, 'test/neg/': test_neg}\n",
        "\n",
        "# Load the data\n",
        "for dataset in sets_dict:\n",
        "  file_list = [f for f in sorted(os.listdir(os.path.join(path, dataset))) if f.endswith('.txt')]\n",
        "  load_text_from_folders(os.path.join(path, dataset), file_list, sets_dict[dataset])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "3t5nCE_xpvx7",
        "tags": []
      },
      "source": [
        "Convert into Pandas dataframes. Pandas is a virtual spreadsheet with a programmatic API. A ```DataFrame``` is a spreadsheet. We will make a spreadsheet of training data and one for testing data and one with everything together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "editable": false,
        "id": "kovNgXqIIKTP",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "# Concatenate training and testing examples into one dataset\n",
        "TRAIN = pd.concat([pd.DataFrame({'review': train_pos, 'label':1}),\n",
        "                     pd.DataFrame({'review': train_neg, 'label':0})],\n",
        "                     axis=0, ignore_index=True)\n",
        "\n",
        "TEST = pd.concat([pd.DataFrame({'review': test_pos, 'label':1}),\n",
        "                    pd.DataFrame({'review': test_neg, 'label':0})],\n",
        "                    axis=0, ignore_index=True)\n",
        "\n",
        "ALL = pd.concat([TRAIN, TEST])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "CKUFtqERp79N",
        "tags": []
      },
      "source": [
        "Look at the data.\n",
        "\n",
        "This is a summary of the data. We see that the data is balanced between labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "editable": false,
        "id": "0xPnt_CDIKTP",
        "tags": [
          "raises-exception"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0e5306a-c174-4978-8547-61b5cbad6b90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "1    2000\n",
              "0    2000\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "TRAIN.label.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "QlV_TMBLqOYB",
        "tags": []
      },
      "source": [
        "This is the first few rows of the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "editable": false,
        "id": "InVC2IvnIKTQ",
        "tags": [
          "raises-exception"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "0ecde490-7e30-4fe5-94d1-bfc772be8d76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  label\n",
              "0  [bromwel, high, cartoon, comedi, ran, time, pr...      1\n",
              "1  [homeless, houseless, georg, carlin, state, is...      1\n",
              "2  [brilliant, act, lesley, ann, warren, best, dr...      1\n",
              "3  [easili, underr, film, inn, brook, cannon, sur...      1\n",
              "4  [typic, mel, brook, film, much, less, slapstic...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-54f3c61f-e8d7-4238-a171-f915af720557\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[bromwel, high, cartoon, comedi, ran, time, pr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[homeless, houseless, georg, carlin, state, is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[brilliant, act, lesley, ann, warren, best, dr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[easili, underr, film, inn, brook, cannon, sur...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[typic, mel, brook, film, much, less, slapstic...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54f3c61f-e8d7-4238-a171-f915af720557')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-54f3c61f-e8d7-4238-a171-f915af720557 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-54f3c61f-e8d7-4238-a171-f915af720557');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-948d024f-cf4c-428e-898c-fa7a7718bccb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-948d024f-cf4c-428e-898c-fa7a7718bccb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-948d024f-cf4c-428e-898c-fa7a7718bccb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "TRAIN",
              "summary": "{\n  \"name\": \"TRAIN\",\n  \"rows\": 4000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "TRAIN.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "3jP5U6usIKTR",
        "tags": []
      },
      "source": [
        "# Creating a vocabulary file\n",
        "\n",
        "Next, we have to build a vocabulary. This is effectively a look-up table where every unique word in your data set has a corresponding index (an integer).\n",
        "We do this as our machine learning model cannot operate on strings, but only numbers. Each index is used to construct a one-hot vector for each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "editable": false,
        "id": "Oz6hQlCgqckU",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self._word2index = {}\n",
        "        self._word2count = {}\n",
        "        self._index2word = {}\n",
        "        self._n_words = 0\n",
        "\n",
        "    def get_words(self):\n",
        "      return list(self._word2count.keys())\n",
        "\n",
        "    def num_words(self):\n",
        "      return self._n_words\n",
        "\n",
        "    def word2index(self, word):\n",
        "      return self._word2index[word]\n",
        "\n",
        "    def index2word(self, word):\n",
        "      return self._index2word[word]\n",
        "\n",
        "    def word2count(self, word):\n",
        "      return self._word2count[word]\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self._word2index:\n",
        "            self._word2index[word] = self._n_words\n",
        "            self._word2count[word] = 1\n",
        "            self._index2word[self._n_words] = word\n",
        "            self._n_words += 1\n",
        "        else:\n",
        "            self._word2count[word] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "UT7oI7pXw9wA",
        "tags": []
      },
      "source": [
        "Make a vocab object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "editable": false,
        "id": "-FLxtGNgw79Q",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "VOCAB = Vocab(\"imdb\")\n",
        "VOCAB_SIZE = 1000\n",
        "NUM_LABELS = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "ggqgPtCSxAG-",
        "tags": []
      },
      "source": [
        "Load the first ```n``` frequent words in the vocabulary. Do this by sorting by frequency and then truncating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "editable": false,
        "id": "DAiKNELzwC2T",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "# Get word frequency counts\n",
        "word_freq_dict = {}   # key = word, value = frequency\n",
        "for review in ALL['review']:\n",
        "  for word in review:\n",
        "    if word in word_freq_dict:\n",
        "      word_freq_dict[word] += 1\n",
        "    else:\n",
        "      word_freq_dict[word] = 1\n",
        "\n",
        "# Get a list of (word, freq) tuples sorted by frequency\n",
        "kv_list = []  # list of word-freq tuples so can sort\n",
        "for (k,v) in word_freq_dict.items():\n",
        "  kv_list.append((k,v))\n",
        "sorted_kv_list = sorted(kv_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Load top n words in to vocab object\n",
        "for word, freq in sorted_kv_list[:VOCAB_SIZE]:\n",
        "  VOCAB.add_word(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "hvh0yHT_IKTi",
        "tags": []
      },
      "source": [
        "# Naive Bayes\n",
        "Naive Bayes Algorithm is based on the Bayes Rule which describes the probability of an event,\n",
        "based on prior knowledge of conditions that might be related to the event.\n",
        "\n",
        "According to Bayes theorem:\n",
        "\n",
        "\n",
        "```Posterior = likelihood * proposition/evidence```\n",
        "\n",
        "or\n",
        "\n",
        "```P(A|B) = P(B|A) * P(A)/P(B)```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "vELAfeWDRrNq",
        "tags": []
      },
      "source": [
        "Using word presence as features, create an array of features for each review. Each review will thus be an array of size ```len(vocab)``` where each index in the array is a token number and the value in that position is whether the token is present in the review. There will be ```num_rows``` arrays, making a ```num_rows x len(vocab)``` 2D array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "DFf6zYmozBtc",
        "tags": []
      },
      "source": [
        "This function creates a bag of words. It returns a vector where each element is a count of the words in the sentence corresponding to the word index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "editable": false,
        "id": "DKCwfenwIKTw",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "def make_bow(sentence):\n",
        "    vec = torch.zeros(VOCAB_SIZE, dtype=torch.float64)\n",
        "    for word in sentence:\n",
        "        if word not in VOCAB.get_words():\n",
        "            continue\n",
        "        vec[VOCAB.word2index(word)] += 1\n",
        "    return vec.view(1, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "NHweI8gPzOdh",
        "tags": []
      },
      "source": [
        "Prepare data ```X_TRAIN``` is a 2D array of size ```num_reviews x vocab_size``` that contains training data. Each row will be a bag of words, except each index contains a 1 or 0 based on word presence in the example. Each row is a vector of features $\\phi_1 ... \\phi_{|V|}$ assumed to be independent, where $|V|$ is size of the vocabulary. We don't need to know what the features are, only whether they are present in each example in the training set.\n",
        "\n",
        "```X_TEST``` is the same as above but containing testing data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "editable": false,
        "id": "c30F5ZwyIKTw",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "# Vectorize text reviews to numbers\n",
        "# Make empty vectors\n",
        "X_TRAIN = np.zeros((len(TRAIN), VOCAB_SIZE))\n",
        "X_TEST = np.zeros((len(TEST), VOCAB_SIZE))\n",
        "\n",
        "# Load in frequency counts\n",
        "for i, row in TRAIN.iterrows():\n",
        "    X_TRAIN[i] = np.array(make_bow(row['review'])) > 0 # The > 0 converts to presence instead of counts\n",
        "\n",
        "for i, row in TEST.iterrows():\n",
        "    X_TEST[i] = np.array(make_bow(row['review'])) > 0 # The > 0 converts to presence instead of counts\n",
        "\n",
        "# The labels\n",
        "Y_TRAIN = np.array(TRAIN['label'])\n",
        "Y_TEST = np.array(TEST['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "ao16p7_U7Zdz",
        "tags": []
      },
      "source": [
        "What you want to do is to compute probabilities over the training data and then apply those probabilities to the testing examples. Use the Bayes formula to compute $P_{\\rm test}(L_{+}|\\phi_{0:|V|})$ and $P_{\\rm test}(L_{-}|\\phi_{0:|V|})$ for each review. Classify examples based on whether one probability is higher than another. That is, $sign(P_{\\rm test}(L_{+}|\\phi_{0:|V|}) - P_{\\rm test}(L_{-}|\\phi_{0:|V|}))$ indicates a positive review when greater than 0 and a negative review when less than 0.\n",
        "\n",
        "**Hint:** You do not need to implement any loops. Numpy indexing and slicing operations, along with built in functions like `.mean()`, `.sum()`, etc. will allow all operations to be performed on each row of the data in parallel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "Q2ch6U7uNmeN",
        "tags": []
      },
      "source": [
        "Step 1: Compute the positive label condition:\n",
        "$P(L_{+}|\\phi_{0:|V|}) = P(\\phi_{0:|V|}|L_{+})P(L_{+}) / P(\\phi_{0:|V|})$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "editable": true,
        "id": "AwAAV9Zq7YbI",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-4e927fe9f8caf96a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "def prob_pos_given_features(x_train, y_train):\n",
        "  log_probs = np.array([0] * x_train.shape[1])\n",
        "  ### BEGIN SOLUTION\n",
        "  # so we need to compute the posterior using the stuff on the right.\n",
        "\n",
        "  # lets use the recitation video as a guide (https://gtvault-my.sharepoint.com/personal/fginac3_gatech_edu/_layouts/15/stream.aspx?id=%2Fpersonal%2Ffginac3%5Fgatech%5Fedu%2FDocuments%2FCS7650%20Recitations%2FHW2%5FRecitation%2Emp4&nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJPbmVEcml2ZUZvckJ1c2luZXNzIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXciLCJyZWZlcnJhbFZpZXciOiJNeUZpbGVzTGlua0NvcHkifX0&ga=1&referrer=StreamWebApp%2EWeb&referrerScenario=AddressBarCopied%2Eview%2Ea2834419%2D3be9%2D43cd%2Da53d%2D4dc656b2bc69)\n",
        "\n",
        "  #  step 1: get the positive instances for each feature\n",
        "  positive_examples = x_train[y_train == 1]\n",
        "\n",
        "  # step 2: compute likelihood probability -  P(φ_{0:|V|}|L_{+}) and add a smoothing constant so we dont get divide by 0\n",
        "  feature_counts_given_pos = positive_examples.sum(axis=0) + 1\n",
        "  total_pos_examples = positive_examples.shape[0] + 1\n",
        "  likelihood_pos = feature_counts_given_pos / total_pos_examples\n",
        "\n",
        "  # steep 3: now compute prior P(L_{+}) - which is really just the probability of seeing a +/- generally.\n",
        "  prior_pos = np.mean(y_train)\n",
        "\n",
        "  # step 4: calculate evidence P(φ_{0:|V|})\n",
        "  feature_prob = (x_train.sum(axis=0) + 1) / (x_train.shape[0] + 1)\n",
        "\n",
        "  # Step 5: now calc the the posterior probability P(L_{+}|\\phi_{0:|V|}) using Bayes' theorem which is given in the formula above.\n",
        "  # But also dont do * and / since we are in logs. --- log P(L_{+}|\\phi_{0:|V|}) = log P(φ_{0:|V|}|L_{+}) + log P(L_{+}) - log P(φ_{0:|V|})\n",
        "  log_probs = np.log(likelihood_pos) + np.log(prior_pos) - np.log(feature_prob)\n",
        "\n",
        "  ### END SOLUTION\n",
        "  return log_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "5eeKepgkNpNa",
        "tags": []
      },
      "source": [
        "Step 2: Compute the negative label condition:\n",
        "$P(L_{-}|\\phi_{0:|V|}) = P(\\phi_{0:|V|}|L_{-})P(L_{-}) / P(\\phi_{0:|V|})$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "editable": true,
        "id": "nWP9U3BuMess",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-996c909da2d8b8bd",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def prob_neg_given_features(x_train, y_train):\n",
        "  log_probs = np.array([0] * x_train.shape[1])\n",
        "  ### BEGIN SOLUTION\n",
        "  # copy the stuff from positive above and then just flip the stuff\n",
        "\n",
        "  # Step 1: Select positive examples\n",
        "  negative_examples = x_train[y_train == 0]\n",
        "\n",
        "  # step 2: compute likelihood probability -  P(φ_{0:|V|}|L_{-}) and add a smoothing constant so we dont get divide by 0\n",
        "  feature_counts_given_neg = negative_examples.sum(axis=0) + 1\n",
        "  total_neg_examples = negative_examples.shape[0] + 1\n",
        "  likelihood_neg = feature_counts_given_neg / total_neg_examples\n",
        "\n",
        "  # step 3: now compute prior P(L_{-}) -- this shoul dbe the probability of seeing a - generally.\n",
        "  prior_neg = 1 - np.mean(y_train)\n",
        "\n",
        "  # step 4: calculate evidence P(φ_{0:|V|})\n",
        "  feature_prob = (x_train.sum(axis=0) + 1) / (x_train.shape[0] + 1) # add a smoothing constant so we dont get /0 sometimes.\n",
        "\n",
        "\n",
        "  # Step 5: now calc the the posterior probability P(L_{-}|\\phi_{0:|V|}) using Bayes' theorem which is given in the formula above.\n",
        "  # But also dont do * and / since we are in logs. --- log P(L_{-}|\\phi_{0:|V|}) = log P(φ_{0:|V|}|L_{-}) + log P(L_{+}) - log P(φ_{0:|V|})\n",
        "  log_probs = np.log(likelihood_neg) + np.log(prior_neg) - np.log(feature_prob)\n",
        "\n",
        "\n",
        "  ### END SOLUTION\n",
        "  return log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "editable": false,
        "id": "ZXdgx0Ku9sYq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "pos_probs = prob_pos_given_features(X_TRAIN, Y_TRAIN)\n",
        "neg_probs = prob_neg_given_features(X_TRAIN, Y_TRAIN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "q9wbtxDZNuj3",
        "tags": []
      },
      "source": [
        "Step 3: Make a label prediction. Subtract (in log scale) the positive from the negative. If the result is greater than zero then it is a prediction of `+` label. If the result is less thn zero then we make a prediction of `-` label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "editable": true,
        "id": "xaz7KBdd_CD1",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-7b4cde57fd68ac07",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "def naive_bayes(x, pos_probs, neg_probs):\n",
        "  label = 0\n",
        "  ### BEGIN SOLUTION\n",
        "\n",
        "  log_prob_pos = np.sum(x * pos_probs)\n",
        "  log_prob_neg = np.sum(x * neg_probs)\n",
        "\n",
        "  # we want to use np.sign to determine label based on log-probabilities\n",
        "  label = np.sign(log_prob_pos - log_prob_neg)\n",
        "  # need to convert np.sign output to binary class using like -1 -> 0, 1 -> 1... shouldnt be any 0's but those are also 0 i guess.\n",
        "  label = 1 if label == 1 else 0\n",
        "\n",
        "  ### END SOLUTION\n",
        "  return label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "VUOFzFVU_is3",
        "tags": []
      },
      "source": [
        "# Naive Bayes Test (20 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "editable": false,
        "id": "T1rcPgnWP_6V",
        "tags": [
          "raises-exception"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29a6a570-3098-45c4-cb15-0c7d312ea48d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.83725\n",
            "Test A: 20/20\n"
          ]
        }
      ],
      "source": [
        "# student check - accuracies >= 78% will receive full credit (no credit for less than 78%)\n",
        "ag.test_naive_bayes(X_TRAIN, Y_TRAIN, X_TEST, Y_TEST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "hUTymITZIKT5",
        "tags": []
      },
      "source": [
        "# Logistic Regression - Part 1\n",
        "\n",
        "We will be using a neural network to perform logistic regression. We will use word counts as the input feature vector.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "DWKJNb2CXhe5",
        "tags": []
      },
      "source": [
        "Reload the data, but use word counts instead of word presence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "editable": false,
        "id": "sBjsMbDSIKT6",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "# Randomize the data\n",
        "TRAIN = TRAIN.sample(frac=1).reset_index(drop=True)\n",
        "TEST = TEST.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Vectorize text reviews to numbers\n",
        "X_TRAIN = np.zeros((len(TRAIN), VOCAB_SIZE))\n",
        "X_TEST = np.zeros((len(TEST), VOCAB_SIZE))\n",
        "\n",
        "for i, row in TRAIN.iterrows():\n",
        "  X_TRAIN[i] = np.array(make_bow(row['review']))\n",
        "\n",
        "for i, row in TEST.iterrows():\n",
        "  X_TEST[i] = np.array(make_bow(row['review']))\n",
        "\n",
        "Y_TRAIN = np.array(TRAIN['label'])\n",
        "Y_TEST = np.array(TEST['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "mSBC93Ge6F1e",
        "tags": []
      },
      "source": [
        "Make a logistic classifier torch neural network.\n",
        "\n",
        "Complete the constructor and forward function. The net will take an arbitrary number of outputs, but for binary logistic regression, only one is needed because the single output neuron can take a value that is between 0 and 1, with 0 meaning negative sentiment and 1 meaning positive sentiment. There should only be as many parameters as ```num_features x (num_labels-1)``` in binary logistic regression and ```num_features x num_labels``` for multinomial logistic regression.\n",
        "\n",
        "The input will be a one-hot vector of size `vocab_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "editable": true,
        "id": "yeJnVl7NIKT8",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-c6124df74ac9d8f1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "# Defining neural network structure\n",
        "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
        "\n",
        "  def __init__(self, num_labels, vocab_size):\n",
        "    super(BoWClassifier, self).__init__()\n",
        "\n",
        "    # BEGIN SOLUTION\n",
        "    self.linear_layer = nn.Linear(vocab_size, num_labels)\n",
        "\n",
        "    # don't use this because it says I have too many layers.\n",
        "    # ed stem says the trick is to use a function: https://edstem.org/us/courses/58916/discussion/5015566?answer=11589388\n",
        "    # self.sigmoid_layer = nn.Sigmoid()\n",
        "    # END SOLUTION\n",
        "\n",
        "  def forward(self, bow_vec):\n",
        "    # BEGIN SOLUTION\n",
        "    out = torch.sigmoid(self.linear_layer(bow_vec))\n",
        "    # END SOLUTION\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "editable": false,
        "id": "mgPYvkocIKT8",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "# Use one label because the head can signify a 1 or 0 because of the sigmoid.\n",
        "bow_nn_model = BoWClassifier(NUM_LABELS-1, VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "xaBdfHfDQ5oI",
        "tags": []
      },
      "source": [
        "This function should return two tensors. The first, containing training data, shoud be of size ```batch_size x vocab_size``` for the ```i```th batch. The second should be a list of labels of size ```batch_size```. Both tensors should be of type ```dtype=torch.float```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "editable": true,
        "id": "RPxjUwUTPZTq",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-abf634c3a00755c7",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "def get_batch(i, batch_size, x_data, y_data):\n",
        "  # Make some empty tensors\n",
        "  x = None\n",
        "  y = None\n",
        "  ### BEGIN SOLUTION\n",
        "\n",
        "  # try to slice into the arrays.\n",
        "  start_idx = i * batch_size # so like 0\n",
        "  end_idx = start_idx + batch_size # to batch_size (non-inclusive)\n",
        "\n",
        "  # Extract the batch data from x_data and y_data\n",
        "  x = torch.tensor(x_data[start_idx:end_idx], dtype=torch.float32)\n",
        "  y = torch.tensor(y_data[start_idx:end_idx], dtype=torch.float32)\n",
        "\n",
        "  ### END SOLUTION\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "2eMh98LwOwv_",
        "tags": []
      },
      "source": [
        "# Logistic Regression - Part 1 Test (20 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "editable": false,
        "id": "tp-B0qeXNnyo",
        "tags": [
          "raises-exception"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b48add28-5292-4add-8243-b920e88d1b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape looks good!\n",
            "Test B: 5/5\n"
          ]
        }
      ],
      "source": [
        "# student check\n",
        "ag.test_batch_output_shape(get_batch, X_TRAIN, Y_TRAIN, VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "editable": false,
        "id": "FdPVSLtrN_2k",
        "tags": [
          "raises-exception"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5936f6ba-3d58-49a7-c2a5-951c947b600f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has the expected number of layers.\n",
            "Test C: 5/5\n",
            "First layer is a Linear layer.\n",
            "Test D: 5/5\n"
          ]
        }
      ],
      "source": [
        "# student check - your model must have the expected number of layers to receive full credit, no credit otherwise\n",
        "ag.check_bow_architecture(bow_nn_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "editable": false,
        "id": "FOYrmRVUj7g7",
        "tags": [
          "raises-exception"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63efae2a-9023-4f0c-bbb6-b98294eafc03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward pass output shape looks good!\n",
            "Test E: 5/5\n"
          ]
        }
      ],
      "source": [
        "# student check\n",
        "ag.test_forward_pass_shape(X_TRAIN, Y_TRAIN, bow_nn_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "rN5v9cjKif_H",
        "tags": []
      },
      "source": [
        "# Logistic Regression - Part 2\n",
        "\n",
        "Create a dataset as an array of (X_train, label).\n",
        "\n",
        "Complete ```get_batch(i)``` and set ```batch_size``` and ```num_epochs```.\n",
        "\n",
        "Training loop will call ```get_batch()``` with the iteration number and do everything else.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "editable": false,
        "id": "paJEFuigIKT-",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "def train(model, train_data, test_data, epochs, batch_size):\n",
        "  n_iter = len(train_data) // batch_size\n",
        "  print(n_iter, 'batches per epoch')\n",
        "  # Loss Function\n",
        "  loss_function = nn.BCELoss()\n",
        "  # Optimizer initlialization\n",
        "  optimizer = optim.SGD(bow_nn_model.parameters(), lr=0.1)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    # Make BOW vector for input features and target label\n",
        "    for i in range(n_iter):\n",
        "      x, y = get_batch(i, batch_size, train_data, test_data)\n",
        "\n",
        "      # Step 3. Run the forward pass.\n",
        "      y_hat = model(x)\n",
        "      y_hat = y_hat.reshape(-1)\n",
        "\n",
        "      # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "      loss = loss_function(y_hat,y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if (epoch+1)%10 == 0 and (i+1) == n_iter:\n",
        "        print('epoch:', epoch+1,',loss =',loss.item(), ', training accuracy =',(torch.round(y_hat)==y).float().mean())\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "editable": true,
        "id": "gzD90s3KQgJd",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "# It's ok to modify this cell.\n",
        "BATCH_SIZE = 100\n",
        "N_EPOCHS = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "editable": false,
        "id": "Tv8Lqg8T949T",
        "scrolled": true,
        "tags": [
          "raises-exception"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20f06cc4-4721-4ab3-f461-267a366e64c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40 batches per epoch\n",
            "epoch: 10 ,loss = 0.27755504846572876 , training accuracy = tensor(0.9300)\n",
            "epoch: 20 ,loss = 0.22101137042045593 , training accuracy = tensor(0.9500)\n",
            "epoch: 30 ,loss = 0.19213198125362396 , training accuracy = tensor(0.9600)\n",
            "epoch: 40 ,loss = 0.17360422015190125 , training accuracy = tensor(0.9700)\n",
            "epoch: 50 ,loss = 0.1603197455406189 , training accuracy = tensor(0.9800)\n",
            "epoch: 60 ,loss = 0.15019173920154572 , training accuracy = tensor(0.9800)\n",
            "epoch: 70 ,loss = 0.14217042922973633 , training accuracy = tensor(0.9700)\n",
            "epoch: 80 ,loss = 0.13564561307430267 , training accuracy = tensor(0.9700)\n",
            "epoch: 90 ,loss = 0.13022813200950623 , training accuracy = tensor(0.9700)\n",
            "epoch: 100 ,loss = 0.12565425038337708 , training accuracy = tensor(0.9700)\n",
            "epoch: 110 ,loss = 0.12173765152692795 , training accuracy = tensor(0.9700)\n",
            "epoch: 120 ,loss = 0.11834284663200378 , training accuracy = tensor(0.9700)\n",
            "epoch: 130 ,loss = 0.11536899209022522 , training accuracy = tensor(0.9700)\n",
            "epoch: 140 ,loss = 0.11273956298828125 , training accuracy = tensor(0.9700)\n",
            "epoch: 150 ,loss = 0.11039546877145767 , training accuracy = tensor(0.9700)\n",
            "epoch: 160 ,loss = 0.10829029232263565 , training accuracy = tensor(0.9800)\n",
            "epoch: 170 ,loss = 0.10638724267482758 , training accuracy = tensor(0.9800)\n",
            "epoch: 180 ,loss = 0.10465672612190247 , training accuracy = tensor(0.9800)\n",
            "epoch: 190 ,loss = 0.10307451337575912 , training accuracy = tensor(0.9800)\n",
            "epoch: 200 ,loss = 0.10162074863910675 , training accuracy = tensor(0.9800)\n",
            "epoch: 210 ,loss = 0.10027892887592316 , training accuracy = tensor(0.9800)\n",
            "epoch: 220 ,loss = 0.09903527051210403 , training accuracy = tensor(0.9800)\n",
            "epoch: 230 ,loss = 0.09787822514772415 , training accuracy = tensor(0.9800)\n",
            "epoch: 240 ,loss = 0.0967978835105896 , training accuracy = tensor(0.9800)\n",
            "epoch: 250 ,loss = 0.09578575193881989 , training accuracy = tensor(0.9800)\n",
            "epoch: 260 ,loss = 0.09483477473258972 , training accuracy = tensor(0.9800)\n",
            "epoch: 270 ,loss = 0.0939386785030365 , training accuracy = tensor(0.9800)\n",
            "epoch: 280 ,loss = 0.09309200942516327 , training accuracy = tensor(0.9800)\n",
            "epoch: 290 ,loss = 0.09229016304016113 , training accuracy = tensor(0.9800)\n",
            "epoch: 300 ,loss = 0.09152901917695999 , training accuracy = tensor(0.9800)\n",
            "epoch: 310 ,loss = 0.09080500900745392 , training accuracy = tensor(0.9900)\n",
            "epoch: 320 ,loss = 0.09011496603488922 , training accuracy = tensor(0.9900)\n",
            "epoch: 330 ,loss = 0.08945619314908981 , training accuracy = tensor(0.9900)\n",
            "epoch: 340 ,loss = 0.08882617205381393 , training accuracy = tensor(0.9900)\n",
            "epoch: 350 ,loss = 0.08822273463010788 , training accuracy = tensor(0.9900)\n",
            "epoch: 360 ,loss = 0.08764397352933884 , training accuracy = tensor(0.9900)\n",
            "epoch: 370 ,loss = 0.08708815276622772 , training accuracy = tensor(0.9900)\n",
            "epoch: 380 ,loss = 0.0865536481142044 , training accuracy = tensor(0.9900)\n",
            "epoch: 390 ,loss = 0.08603911101818085 , training accuracy = tensor(0.9900)\n",
            "epoch: 400 ,loss = 0.08554326742887497 , training accuracy = tensor(0.9900)\n",
            "epoch: 410 ,loss = 0.08506492525339127 , training accuracy = tensor(0.9900)\n",
            "epoch: 420 ,loss = 0.08460299670696259 , training accuracy = tensor(0.9900)\n",
            "epoch: 430 ,loss = 0.08415654301643372 , training accuracy = tensor(0.9900)\n",
            "epoch: 440 ,loss = 0.08372463285923004 , training accuracy = tensor(0.9900)\n",
            "epoch: 450 ,loss = 0.0833064541220665 , training accuracy = tensor(0.9900)\n",
            "epoch: 460 ,loss = 0.08290128409862518 , training accuracy = tensor(0.9900)\n",
            "epoch: 470 ,loss = 0.08250825852155685 , training accuracy = tensor(0.9900)\n",
            "epoch: 480 ,loss = 0.08212690055370331 , training accuracy = tensor(0.9900)\n",
            "epoch: 490 ,loss = 0.08175656944513321 , training accuracy = tensor(0.9900)\n",
            "epoch: 500 ,loss = 0.08139664679765701 , training accuracy = tensor(0.9900)\n",
            "epoch: 510 ,loss = 0.08104662597179413 , training accuracy = tensor(0.9900)\n",
            "epoch: 520 ,loss = 0.08070598542690277 , training accuracy = tensor(0.9900)\n",
            "epoch: 530 ,loss = 0.08037424087524414 , training accuracy = tensor(0.9900)\n",
            "epoch: 540 ,loss = 0.0800510048866272 , training accuracy = tensor(0.9900)\n",
            "epoch: 550 ,loss = 0.0797358900308609 , training accuracy = tensor(0.9900)\n",
            "epoch: 560 ,loss = 0.07942838221788406 , training accuracy = tensor(0.9900)\n",
            "epoch: 570 ,loss = 0.07912827283143997 , training accuracy = tensor(0.9900)\n",
            "epoch: 580 ,loss = 0.07883518934249878 , training accuracy = tensor(0.9900)\n",
            "epoch: 590 ,loss = 0.07854875922203064 , training accuracy = tensor(0.9900)\n",
            "epoch: 600 ,loss = 0.07826870679855347 , training accuracy = tensor(0.9900)\n",
            "epoch: 610 ,loss = 0.07799477875232697 , training accuracy = tensor(0.9900)\n",
            "epoch: 620 ,loss = 0.07772666215896606 , training accuracy = tensor(0.9900)\n",
            "epoch: 630 ,loss = 0.07746412605047226 , training accuracy = tensor(0.9900)\n",
            "epoch: 640 ,loss = 0.07720697671175003 , training accuracy = tensor(0.9900)\n",
            "epoch: 650 ,loss = 0.07695497572422028 , training accuracy = tensor(0.9900)\n",
            "epoch: 660 ,loss = 0.07670783996582031 , training accuracy = tensor(0.9900)\n",
            "epoch: 670 ,loss = 0.07646546512842178 , training accuracy = tensor(0.9900)\n",
            "epoch: 680 ,loss = 0.07622765004634857 , training accuracy = tensor(0.9900)\n",
            "epoch: 690 ,loss = 0.07599417120218277 , training accuracy = tensor(0.9900)\n",
            "epoch: 700 ,loss = 0.07576489448547363 , training accuracy = tensor(0.9900)\n",
            "epoch: 710 ,loss = 0.07553964108228683 , training accuracy = tensor(0.9900)\n",
            "epoch: 720 ,loss = 0.07531828433275223 , training accuracy = tensor(0.9900)\n",
            "epoch: 730 ,loss = 0.07510072737932205 , training accuracy = tensor(0.9900)\n",
            "epoch: 740 ,loss = 0.07488670945167542 , training accuracy = tensor(0.9900)\n",
            "epoch: 750 ,loss = 0.07467617839574814 , training accuracy = tensor(0.9900)\n",
            "epoch: 760 ,loss = 0.07446905970573425 , training accuracy = tensor(0.9900)\n",
            "epoch: 770 ,loss = 0.07426513731479645 , training accuracy = tensor(0.9900)\n",
            "epoch: 780 ,loss = 0.07406434416770935 , training accuracy = tensor(0.9900)\n",
            "epoch: 790 ,loss = 0.07386663556098938 , training accuracy = tensor(0.9900)\n",
            "epoch: 800 ,loss = 0.0736718401312828 , training accuracy = tensor(0.9900)\n",
            "epoch: 810 ,loss = 0.07347989082336426 , training accuracy = tensor(0.9900)\n",
            "epoch: 820 ,loss = 0.07329066097736359 , training accuracy = tensor(0.9900)\n",
            "epoch: 830 ,loss = 0.07310410588979721 , training accuracy = tensor(0.9900)\n",
            "epoch: 840 ,loss = 0.07292009145021439 , training accuracy = tensor(0.9900)\n",
            "epoch: 850 ,loss = 0.07273860275745392 , training accuracy = tensor(0.9900)\n",
            "epoch: 860 ,loss = 0.07255959510803223 , training accuracy = tensor(0.9900)\n",
            "epoch: 870 ,loss = 0.07238293439149857 , training accuracy = tensor(0.9900)\n",
            "epoch: 880 ,loss = 0.07220849394798279 , training accuracy = tensor(0.9900)\n",
            "epoch: 890 ,loss = 0.07203631103038788 , training accuracy = tensor(0.9900)\n",
            "epoch: 900 ,loss = 0.07186634838581085 , training accuracy = tensor(0.9900)\n",
            "epoch: 910 ,loss = 0.07169842720031738 , training accuracy = tensor(0.9900)\n",
            "epoch: 920 ,loss = 0.07153260707855225 , training accuracy = tensor(0.9900)\n",
            "epoch: 930 ,loss = 0.07136868685483932 , training accuracy = tensor(0.9900)\n",
            "epoch: 940 ,loss = 0.07120674103498459 , training accuracy = tensor(0.9900)\n",
            "epoch: 950 ,loss = 0.07104668766260147 , training accuracy = tensor(0.9900)\n",
            "epoch: 960 ,loss = 0.07088842242956161 , training accuracy = tensor(0.9900)\n",
            "epoch: 970 ,loss = 0.07073197513818741 , training accuracy = tensor(0.9900)\n",
            "epoch: 980 ,loss = 0.07057727128267288 , training accuracy = tensor(0.9900)\n",
            "epoch: 990 ,loss = 0.07042429596185684 , training accuracy = tensor(0.9900)\n",
            "epoch: 1000 ,loss = 0.0702730342745781 , training accuracy = tensor(0.9900)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    bow_nn_model = train(bow_nn_model, X_TRAIN, Y_TRAIN, N_EPOCHS, BATCH_SIZE)\n",
        "except:\n",
        "    print(\"Training failed. Please check your code.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "G4D7NpQlkBGf",
        "tags": []
      },
      "source": [
        "# Logistic Regression - Part 2 Test (20 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "editable": false,
        "id": "qW0uESVguCuL",
        "tags": [
          "raises-exception"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6ab5dd0-6705-4529-e232-b89204fbc400"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.83      0.82      2000\n",
            "           1       0.83      0.82      0.82      2000\n",
            "\n",
            "    accuracy                           0.82      4000\n",
            "   macro avg       0.82      0.82      0.82      4000\n",
            "weighted avg       0.82      0.82      0.82      4000\n",
            "\n",
            "Accuracy:  0.8\n",
            "Test F: 20/20\n"
          ]
        }
      ],
      "source": [
        "# student check - accuracies >= 78% will receive full credit (no credit for less than 78%)\n",
        "ag.test_model_accuracy_lr(TEST, bow_nn_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "DphmlDCk5sr5",
        "tags": []
      },
      "source": [
        "# Multinomial Regression\n",
        "\n",
        "Load data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "editable": false,
        "id": "SIiy4PBJVh7h",
        "scrolled": true,
        "tags": [
          "raises-exception"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2fc510-e981-4cc4-eb12-260cfdda909e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "editable": false,
        "id": "GrcwUiN-VfSf",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "gqcveFex52Fy",
        "tags": []
      },
      "source": [
        "Unlike earlier, we will use a pre-defined set of embeddings, called [GLoVe](https://nlp.stanford.edu/projects/glove/). GLoVe replaces every word with a 100-dimensional vector of floating point values. The advantage of this is that words with similar semantic meanings will have similar vectors. This is important because the vocabulary size of the corpus we will use is 400,000.\n",
        "\n",
        "For the assigment, instead of getting a one-hot vector for each word, the neural network will get a `batch_size x num_words x 100` tensor containing floating point values.\n",
        "\n",
        "Download the GLoVe embedding vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "editable": false,
        "id": "7zIXPea4pZCi",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "import gensim.downloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "editable": false,
        "id": "-_CNat0Qpf_b",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')\n",
        "VOCAB_SIZE = len(glove_vectors.vectors)\n",
        "EMBEDDING_DIM = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "jRcY0APp-9YI",
        "tags": []
      },
      "source": [
        "This function will embed the dataset into sequences of 100-dimension vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "editable": false,
        "id": "Yo6wUdb9s97G",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "# pad dataset to a maximum review length in words\n",
        "MAX_LEN = 50\n",
        "\n",
        "def get_glove_seq(review, max_len):\n",
        "  seq = np.zeros((max_len, 100))\n",
        "  for i, word in enumerate(review):\n",
        "    if i < max_len and word in glove_vectors:\n",
        "      seq[i] = glove_vectors[word]\n",
        "  return seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "editable": false,
        "id": "ZtpvG4mJnwMf",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "news_data_train = load_dataset(\"ag_news\", split=\"train\").shuffle()\n",
        "news_data_test = load_dataset(\"ag_news\", split=\"test\").shuffle()\n",
        "NEWS_TRAIN = pd.DataFrame(news_data_train)[:5000]\n",
        "NEWS_TEST = pd.DataFrame(news_data_test)[:5000]\n",
        "NUM_LABELS = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "editable": false,
        "id": "pmx1OElZaAki",
        "tags": [
          "raises-exception"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "f6afa4bc-e024-47e6-e194-984614259900"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0  Vikings RB Onterrio Smith suspended Minneapoli...      1\n",
              "1  Owen tastes Real joy ENGLAND striker Michael O...      1\n",
              "2  Despite struggles at plate, Boone wins Gold Gl...      1\n",
              "3  Palm OS for Linux? \\\\Today was a strange day f...      3\n",
              "4  U.S. Army aims to halt paperwork with IBM syst...      3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6e0bbbd6-fea8-40bb-ad25-ab7f11b00736\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Vikings RB Onterrio Smith suspended Minneapoli...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Owen tastes Real joy ENGLAND striker Michael O...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Despite struggles at plate, Boone wins Gold Gl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Palm OS for Linux? \\\\Today was a strange day f...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>U.S. Army aims to halt paperwork with IBM syst...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e0bbbd6-fea8-40bb-ad25-ab7f11b00736')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6e0bbbd6-fea8-40bb-ad25-ab7f11b00736 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6e0bbbd6-fea8-40bb-ad25-ab7f11b00736');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6e7fefd8-3d2e-425d-b949-0b7ac7206186\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6e7fefd8-3d2e-425d-b949-0b7ac7206186')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6e7fefd8-3d2e-425d-b949-0b7ac7206186 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "NEWS_TEST",
              "summary": "{\n  \"name\": \"NEWS_TEST\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"Streaking Astros Clock Reds 11-5 (AP) AP - Astros pitcher Brandon Backe hit his first career homer, a two-run shot, and allowed one run in seven innings to keep Houston in the thick of the NL wild-card chase with an 11-5 rout of the Cincinnati Reds on Monday.\",\n          \"Philippine President Says Country Faces Fiscal Crisis Philippine president Gloria Arroyo warned that her country is in the midst of a fiscal crisis. A report by economists at the University of the Philippines said the country faces economic collapse \",\n          \"Stocks Sink on Coke's Gloomy Forecast NEW YORK - Stocks headed lower Wednesday after beverage giant Coca-Cola Co. issued a gloomy forecast, and a lower-than-expected reading on industrial production for August threw the nation's broader economic outlook into question...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "NEWS_TEST.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "e5IzjEK0Ixxl",
        "tags": []
      },
      "source": [
        "Train/Test Sets using GloVe embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "editable": false,
        "id": "-I8PSBk-I6qM",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "# Vectorize text reviews to numbers\n",
        "X_NEWS_TRAIN = np.zeros((len(NEWS_TRAIN), MAX_LEN, 100))\n",
        "X_NEWS_TEST = np.zeros((len(NEWS_TEST), MAX_LEN, 100))\n",
        "\n",
        "for i, row in NEWS_TRAIN.iterrows():\n",
        "  X_NEWS_TRAIN[i] = get_glove_seq(tokenize_and_clean(row['text'], stem_and_remove_stop_words=False), MAX_LEN)\n",
        "\n",
        "for i, row in NEWS_TEST.iterrows():\n",
        "  X_NEWS_TEST[i] = get_glove_seq(tokenize_and_clean(row['text'], stem_and_remove_stop_words=False), MAX_LEN)\n",
        "\n",
        "Y_NEWS_TRAIN = np.array(NEWS_TRAIN['label'])\n",
        "Y_NEWS_TEST = np.array(NEWS_TEST['label'])\n",
        "NUM_LABELS = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "editable": true,
        "id": "i1-mK1-EPbQL",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-64776db849e2cffa",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "# Defining neural network structure\n",
        "class MultinomialBoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
        "  def __init__(self, max_word_len, embedding_dim, num_labels):\n",
        "    super(MultinomialBoWClassifier, self).__init__()\n",
        "    self.max_word_len = max_word_len\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.num_labels = num_labels\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    # from the assignment ....\n",
        "    # \"instead of getting a one-hot vector for each word\"\n",
        "    # \" the neural network will get a `batch_size x num_words x 100` tensor containing floating point values#\n",
        "    # so we need to now change the input - we can ignore batch size. and the input dimension is\n",
        "    # a giant blob of the encoded words - where each word is  encoded as 100 dimensions.\n",
        "    # and there can be max MAX_LEN words in the review - we ignore the rest.... so that should give\n",
        "    # something like MAX_LEN * 100 which is the embedding dim.\n",
        "\n",
        "    # and then we just map that to labels.\n",
        "    self.linear = nn.Linear (max_word_len * embedding_dim, num_labels)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    ### END SOLUTION\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = None\n",
        "    ### BEGIN SOLUTION\n",
        "    embedded = x.view(x.size(0), -1)  # collapse the tensor to (batch_size, num_words * embedding_dim)\n",
        "    logits = self.linear(embedded) # the probability logits from the mapping\n",
        "    out = self.softmax(logits) # then we softmax to see which of the classes is the most likely.\n",
        "\n",
        "    ### END SOLUTION\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "editable": false,
        "id": "_eVavHp7TOxv",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "multibow_model = MultinomialBoWClassifier(max_word_len=MAX_LEN, embedding_dim=EMBEDDING_DIM, num_labels=NUM_LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "editable": false,
        "id": "wc0JD91dP9-a",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "def train(model, x_train_data, y_train_data, epochs, batch_size, lr, weight_decay):\n",
        "  print('Training Started!')\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  n_iter = len(x_train_data) // batch_size\n",
        "  print(n_iter, 'batches per epoch')\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    num_correct = 0\n",
        "    total_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for i in range(n_iter):\n",
        "      x, y = get_batch(i, batch_size, x_train_data, y_train_data)\n",
        "      x = x\n",
        "      y = y.long()\n",
        "\n",
        "      y_hat = model(x)\n",
        "      loss = criterion(y_hat, y)\n",
        "      total_loss += loss\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if (epoch+1)%10 == 0 and (i+1) == n_iter:\n",
        "        print('epoch:', epoch+1,',loss =',loss.item(), ', training accuracy =',(y_hat.argmax(dim=1)==y).float().mean().item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "editable": true,
        "id": "POVLJlC0_mEl",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "# It's ok to modify this cell.\n",
        "BATCH_SIZE = 10\n",
        "N_EPOCHS = 100\n",
        "LEARNING_RATE = 2e-3\n",
        "WEIGHT_DECAY = 1e-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "editable": false,
        "id": "uhVQB2RCbPj7",
        "scrolled": true,
        "tags": [
          "raises-exception"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf017c5a-dae9-4ba7-a03a-5791973b5f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Started!\n",
            "500 batches per epoch\n",
            "epoch: 10 ,loss = 0.8571216464042664 , training accuracy = 0.8999999761581421\n",
            "epoch: 20 ,loss = 0.9062027931213379 , training accuracy = 0.800000011920929\n",
            "epoch: 30 ,loss = 0.8676505088806152 , training accuracy = 0.8999999761581421\n",
            "epoch: 40 ,loss = 0.8847274780273438 , training accuracy = 0.8999999761581421\n",
            "epoch: 50 ,loss = 0.8728540539741516 , training accuracy = 0.8999999761581421\n",
            "epoch: 60 ,loss = 0.8761332631111145 , training accuracy = 0.8999999761581421\n",
            "epoch: 70 ,loss = 0.926604151725769 , training accuracy = 0.800000011920929\n",
            "epoch: 80 ,loss = 0.8707021474838257 , training accuracy = 0.8999999761581421\n",
            "epoch: 90 ,loss = 0.8954954147338867 , training accuracy = 0.8999999761581421\n",
            "epoch: 100 ,loss = 0.900578498840332 , training accuracy = 0.8999999761581421\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    train(multibow_model, X_NEWS_TRAIN, Y_NEWS_TRAIN, N_EPOCHS, BATCH_SIZE, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "# except:\n",
        "#     print(\"Training failed. Please check your code.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Training failed. Please check your code.\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "50_KqANXCtbs",
        "tags": []
      },
      "source": [
        "# Multinomial Regression - Test (40 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "editable": false,
        "id": "Ltz2NZ_4u9Cr",
        "tags": [
          "raises-exception"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ac051b-5691-4943-9849-28dce871ec09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8299999833106995\n",
            "Test G: 40/40\n"
          ]
        }
      ],
      "source": [
        "# student check - accuracies >= 80% will receive full credit (no credit for less than 80%)\n",
        "ag.test_model_accuracy_mr(X_NEWS_TEST, Y_NEWS_TEST, multibow_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "hKEYvlbhS7n9",
        "tags": []
      },
      "source": [
        "# Grading\n",
        "\n",
        "Please submit this .ipynb file to Canvas for grading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "editable": false,
        "tags": [],
        "id": "jOkxJpPAqseK"
      },
      "source": [
        "## Final Grade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "editable": false,
        "tags": [
          "raises-exception"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkNLPE6nqseK",
        "outputId": "b2caf24d-1065-42f6-8133-245677b6aa7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your projected points for this assignment is 225/100.\n",
            "\n",
            "NOTE: THIS IS NOT YOUR FINAL GRADE. YOUR FINAL GRADE FOR THIS ASSIGNMENT WILL BE AT LEAST 225 OR MORE, BUT NOT LESS\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# student check\n",
        "ag.FINAL_GRADE()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "editable": false,
        "tags": [],
        "id": "BnDpotRpqseK"
      },
      "source": [
        "## Notebook Runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "editable": false,
        "tags": [
          "raises-exception"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAAqSBO3qseK",
        "outputId": "30465ba0-dd74-4b9f-db8a-a0f30b3a93ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notebook execution time in minutes = 4.43054925998052\n"
          ]
        }
      ],
      "source": [
        "# end time - notebook execution\n",
        "end_nb = time.time()\n",
        "# print notebook execution time in minutes\n",
        "print(\"Notebook execution time in minutes =\", (end_nb - start_nb)/60)\n",
        "# warn student if notebook execution time is greater than 30 minutes\n",
        "if (end_nb - start_nb)/60 > 30:\n",
        "  print(\"WARNING: Notebook execution time is greater than 30 minutes. Your submission may not complete auto-grading on Gradescope. Please optimize your code to reduce the notebook execution time.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "bento_stylesheets": {
      "bento/extensions/flow/main.css": true,
      "bento/extensions/kernel_selector/main.css": true,
      "bento/extensions/kernel_ui/main.css": true,
      "bento/extensions/new_kernel/main.css": true,
      "bento/extensions/system_usage/main.css": true,
      "bento/extensions/theme/main.css": true
    },
    "captumWidgetMessage": {},
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "dataExplorerConfig": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "last_base_url": "https://11238.od.fbinfra.net/",
    "last_kernel_id": "8832c409-9272-44ac-a889-c233991c3bb0",
    "last_msg_id": "354bb3e2-f350a4bd64779ec7f6ce4382_4332",
    "last_server_session_id": "5625d4d2-18a1-473d-bf89-0f1a5ecc7c7b",
    "outputWidgetContext": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}